\chapter{Conclusion}
\label{ch:conclusion}
In this thesis, the issue of reference parsing to support token-level field labeling of reference strings has been explored. By integrating manually crafted feature engineering and the newest subword embedding technology, we aimed to provide robust sequence labeling models to tackle the high variability and noise levels in citation format. Properly parsing references is a highly pertinent issue for citation network construction using automated methods, academic metadata retrieval, and scientific information retrieval.

The proposed system combined traditional sequence labeling models like Conditional Random Fields (CRFs) with neural models like Bidirectional LSTM (BiLSTM) networks and more recent Transformer-based encoders. One of the major contributions of this research was the creation and integration of a rich set of handcrafted features, based on AnyStyle parsing techniques, that captured fine-grained linguistic, structural, and positional patterns in reference strings. Apart from these features, the use of Byte-Pair Embeddings (BPEmb) and BERT-inspired embeddings added contextualized subword-level representations, significantly enhancing model understanding of citation elements.

Heavy experimentation revealed that classical CRF models, when combined with both BPEmb embeddings and hand-crafted features, were as good as deeper neural architectures. The BiLSTM + CRF model also improved generalization to unseen data by capturing sequential dependencies at the expense of extra computational resources and training time. Finally, the pairing of a modern BERT-like encoder, i.e., Linq-Embed-Mistral, with CRF decoding achieved the highest overall weighted F1-score, demonstrating the ability of high-capacity language models to model citation structure without explicit feature engineering.

Comparison against external baselines, including a ChatGPT-4o-powered extraction system and prior work on bibliographic parsing, found the suggested models achieved high performance considering the relatively small synthetic training dataset and lack of domain adaptation. On more difficult multi-token fields like \texttt{I-TITLE}, \texttt{I-AUTHOR}, and \texttt{I-URL}, the models maintained high recall and precision, with both the structural features and deep contextual embeddings found to add complementary value.

Despite these promising outcomes, some limitations remain. The system was trained exclusively on synthetic data (the GIANT dataset) and has yet to be validated on truly noisy, real-world references extracted from diverse document types. Furthermore, the models were evaluated using token-level metrics rather than full-reference reconstruction accuracy, which could be a more practical downstream measure in citation graph building tasks. Resource limitations also restricted the use of larger datasets and broader hyperparameter tuning, particularly for transformer-based models.

Future work can expand the system in several directions. First, applying domain adaptation techniques to transfer models on real-world reference datasets may enhance robustness. Second, using recent advances in retrieval-augmented language models or semi-supervised learning can enhance performance without intensive manual annotation. Finally, extrapolating the models to collectively parse references and deduce citation context (in-text citation extraction) could provide an end-to-end solution to automatic bibliography graph construction.

Overall, this thesis demonstrated that structured feature design in combination with powerful neural embeddings yields good performance on the bibliographic reference parsing task. The intersection of linguistic understanding, statistical learning, and modern deep language models presents a generalizable platform for handling one of the most fundamental bottlenecks to the automated study of scholarly discourse. As scientific ecosystems continue to expand, accurate and scalable reference parsing will remain a fundamental component in scientific discovery of knowledge.
