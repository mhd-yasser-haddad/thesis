\chapter{Related Work}
\label{ch:related}

The problem with bibliographic reference string parsing has been a challenge in the field of information extraction for use inside digital libraries and indexing systems. And over the last couple of years, researchers have explored different approaches to segment and label reference strings more accurately, from handcrafted rule-based systems and feature-engineered machine learning models to modern neural architectures. These various approaches aim to convert normal citation strings into structured metadata fields such as author, title, journal, year, and volume, which are very important for digital indexing, citation networks, and scholarly search systems.

\section[Early Methods and the Rise of Machine Learning]{Early Methods and the Rise\\ of Machine Learning}
The first tries to automate the reference parsing task, relied mainly on regular expressions and template-matching techniques. These methods worked well within limited domains or under specific citation styles but quickly became weak and unreliable in the face of noise, new variations of reference styles started showing up, and multilingual citations. The introduction of probabilistic models marked a significant turning point. Notably, Hidden Markov Models (HMMs) were among the earliest probabilistic sequence models applied to this task~\cite{HMM1165342}.
However, the use of Conditional Random Fields (CRFs) surpassed HMMs because they offered a discriminative framework more suited for structured prediction tasks such as reference segmentation and reference parsing. CRFs were introduced as a general and effective replacement that could capture long-range dependencies and handle random, overlapping features without needing strong independent assumptions~\cite{crf2001}. CRFs were the most widely used solutions in most of the early systems, including the well-cited ParsCite system, which combined a CRF model with a collection of heuristics to deal with pre- and post-processing~\cite{councill-etal-2008-parscit}.
ParsCit, in particular, was implemented as a modular, open-source tool and used 23 hand-engineered features that are token identity, position, punctuation, orthography, and external dictionaries (ParsCit). It gave a strong performance on benchmark datasets such as CORA~\cite{cora1999}, and set a standard for other reference parsing systems. CiteSeerX and Mendeley also used similar techniques with dictionary lookups and CRF-based models~\cite{councill-etal-2008-parscit}~\cite{citeseerx}.

\section{Limitations of Feature-Engineered Systems}
Even though they were successful initially, CRF-based systems have some limitations. First of all, their reliance on hand-engineered features made them very dependent on the domain they’re trained on. Features like name dictionaries or publisher lists don’t generalize well between languages or domains. Second, while CRFs can model local context very easily, they can’t handle more complex, long-range dependencies that could appear in many citation styles. Also, most CRF-based models have been trained on homogeneous corpora (typically English-language, computer science citations), which made them limited to use on real-world, multilingual corpora~\cite{prasad2018neuralparscit}.
Studies such as CERMINE~\cite{cermine} and BibPro~\cite{bibpro} further continued to improve finer feature sets and templates to raise accuracy, yet the limitations remained the same: scalability and generalization. The systems also typically ignored the growing availability of large-scale, unlabeled citation data and instead relied on limited, curated gold-standard datasets.

\section{Transition to Deep Learning Models}
To avoid the mentioned limitations, researchers began exploring neural network models that could potentially learn feature representations from raw text without intermediate manual feature engineering. The neural ParsCit system~\cite{prasad2018neuralparscit} is one of the first models that tried to apply deep learning to solve the reference parsing problem. The researchers on that model used a BiLSTM + CRF architecture, in which both character and word embeddings are used as input to the bidirectional LSTM network, and the output was then decoded using the CRF layer.
This combined approach managed to use the power of both deep and structured models: the BiLSTM managed to capture contextual information in both directions, and the CRF made sure that label sequences were consistent. Neural ParsCit showed better performance than traditional CRF-only models, especially on multilingual and out-of-domain references. More importantly, the researchers trained embedding using large unlabeled citation corpora and showed that representation learning could help models generalize better to unfamiliar styles and vocabularies.
The study also discussed how dictionary-based features used in a system like ParsCit are still vulnerable. It avoided rigid lexicons, and its neural model helped with robustness while handling named entities, low-frequency words, or unusual punctuation. This was a step forward in making parsing systems that would address the diversity of real-world academic citations.

\section[Innovations with Contrastive and Prompt Learning]{Innovations with Contrastive\\ and Prompt Learning}
While BiLSTM and Transformer-based models have been responsible for most of the recent progress in bibliographic reference parsing, Yin and Wang~\cite{contrastive} presented a novel approach that includes contrastive learning and prompt-based learning for segmentation.
\texttt{CONT\_Prompt\_ParsRef} is their model that aims to enhance the robustness of the model, especially in low-resource settings and multilingual citation styles, by making representations more discriminative and more interpretable. The contrastive learning component clusters similar entity tokens in the embedding space and separates dissimilar ones, which helps the model to find the difference between ambiguous or overlapping field types. Meanwhile, the prompt learning mechanism helps the model use templated prompts embedded within the input, which allows it to learn from few-shot examples or external guidance.
The authors validated their methods on a bilingual benchmark dataset containing 12,000 reference strings each for Chinese and English. Compared to BiLSTM + CRF and BERT + CRF baselines, their model achieved more than 96\% F1 scores for both sets of languages. Their study confirmed that both modules (contrastive and prompt learning) made considerable contributions towards the performance of the model. Moreover, their system has good generalization to different citation styles. This work is a breakthrough in the sense that it demonstrates that more recent representation learning techniques from larger NLP research can be easily applied to bibliographic parsing tasks.

\section[Comparative Evaluations and Modern Architectures]{Comparative Evaluations\\ and Modern Architectures}
Following up on Neural ParsCit’s~\cite{prasad2018neuralparscit} observations, recent work has focused on the task of comparatively assessing modern NLP architectures for reference parsing. In particular, Cuéllar Hidalgo et al. (2024)~\cite{ArchComapre} conducted an empirical comparison of three well-known architectures: CRF, BiLSTM + CRF, and Transformer + CRF. Their study used the gigantic GIANT corpus~\cite{giant} (comprising over 900 million annotated references across 1500+ citation styles) for training and the well-established CORA corpus~\cite{cora1999} for testing.
Their findings once more validated the superiority of BiLSTM + CRF over pure CRF models in identifying complex syntactic structures in reference strings. What was more interesting, Transformers-based models showed competitive performance but did poorly in cases with limited labeled data or noisy labels, which indicated that even with their theoretical advantages, Transformers still need a lot of careful tuning or more training data to outperform BiLSTMs in this use case.
Perhaps one of their most significant contribution was to standardize preprocessing and consistent testing for all models. This way, it helped us to know that performance differences were because of the architectural differences between the models and not the inconsistencies in the data. These experiments also pointed out the importance of embedding techniques, using Byte-Pair Encoding with character-level features, which enhanced the performance of all models in handling token-level irregularities, such as hyphenated names or Roman numeral volume numbers.

\section{Other Tools and Alternative Approaches}
In addition to traditional and neural sequence models, several other systems have explored new directions. The ParsRec approach~\cite{ParsRec} suggested a meta-learning system that proposes the best parser (out of a set of candidate systems) for a specific reference string, according to metadata and structural information. This is part of a growing tendency towards ensemble and hybrid solutions that take advantage of the strengths of different tools.
In addition, software like GROBID~\cite{grobid} and AnyStyle~\cite{anystyle} have become very popular due to their simplicity and deployment in digital library pipelines. AnyStyle is especially valuable because it employs a hybrid approach with the utilization of hand-crafted features that serve as input into a CRF model, and simple training on proprietary datasets. Its adaptability and low-resource suitability make it suitable for libraries that have specialized citation formats or non-English content.

\section{Summary and Open Challenges}
In short, the field of bibliographic reference string parsing has progressed from rule-based and HMM to CRFs and, more recently, to neural models such as BiLSTM and transformer models. Each has addressed problems of the previous generation, from lack of generalization to dependence on hand-engineered features.
But there remain issues, most systems get trained and tested on English-language datasets mostly, and noisy or OCR-extracted references are still a concern. While neural models generalize more, they are data hungry and may not work well in low-resource environments. Finally, deployment in real workflows (citation indexing, digital repositories) required robust tools that compromise between accuracy, speed, and flexibility.
In our work, we attempt to further explore this balance by re-examining the value of hand-engineered features, like in models such as AnyStyle, and putting their cooperation with modern neural frameworks to the test. We hope that by integrating structured, interpretable features into data-driven learning, we can build models that are both efficient and practical in a wide range of bibliographic scenarios.
