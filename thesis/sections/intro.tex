\chapter{Introduction}
\label{ch:intro}

In any academic research, citations are a vital thing because they help us to credit previous work, illustrate how the ideas provided in the work relate to others, and help whoever reads the work to explore similar ideas and research. In the majority of publications, this task is usually set as a reference string that contains structured segments at the end of the paper (or any type of work). These strings are filled with important metadata about the work, like the author names, article title, publication year, journal names, and more. Being able to interpret them is an important task that would help with building citation networks, tracking the impact of research, and keeping a clean digital library that we can search through it~\cite{councill-etal-2008-parscit}~\cite{prasad2018neuralparscit}.

But parsing reference strings isn’t an easy task, while us humans can take a look at a reference and understand its structure in a fast way, machines struggle. One of the reasons is that there are multiple varieties of citation styles–APA, MLA, IEEE, and more. Even sometimes, authors can use the same style in an inconsistent way. Other reasons that could throw off a machine from identifying a reference are typos, abbreviations, missing punctuation, or format switching mid-paper~\cite{councill-etal-2008-parscit}~\cite{prasad2018neuralparscit}. Named entities like author names or journal titles constantly change, and new citation styles continue to appear, which makes dictionary-based or rule-based approaches weak and error-prone~\cite{prasad2018neuralparscit}.

To solve this problem, researchers have turned to machine learning and treated the reference parsing problem as a \textbf{sequence labeling problem}. Which means that we can treat each word or token as part of a sequence, and we try to label it with the field it belongs to– like “Author”, “Title”, or “Year”. Earlier methods relied on models like \textbf{Hidden Markov Models (HMMs)} and especially \textbf{Conditional Random Fields (CRFs)}, which capture local dependencies in sequences and have been used a lot in the natural language processing field for structured predictions~\cite{HMM1165342}~\cite{crf2001}.

One of the most well-known tools built using these ideas is \textbf{ParsCit} (Peng and McCallum, 2004;~\cite{councill-etal-2008-parscit}), a system that uses both CRFs and heuristic post-processing. Others, like CiteSeerX~\cite{citeseerx}, and Mendeley, have adopted similar methods to power large-scale academic search and indexing~\cite{prasad2018neuralparscit}. These systems allowed us to see how a supervised learning model could generalize across a range of citation formats, if they were given good training data.
Most recently, deep learning methods have entered the picture. \textbf{BiLSTM + CRF} models can combine both the sequential context modeling of BiLSTMs and the prediction power of CRFs. These types of models have shown strong results in capturing long-distance dependencies often found in complex reference strings. And in a newer architecture, \textbf{Transformer-based} models that use self-attention have made them powerful tools for sequence tasks, even though they are data-hungry models~\cite{opensourcebib}~\cite{Syntheticvreal}~\cite{jain2023entityextraction}~\cite{annotatedcorpus}.

Several studies have compared these models directly. One, by~\cite{opensourcebib}, evaluated different approaches like CRFs, LSTMs, rules, and templates under the same data conditions. They found that CRFs remained solid performers, especially when training data was limited, but LSTM + CRF models were superior in accuracy. Another study~\cite{Syntheticvreal}, compared real and synthetic training data for this task and found that synthetic data can be useful, if generated carefully.
Despite these advances, the problem hasn’t been solved. Reference styles are still not very well structured and hard to annotate, and some models can handle different edge cases better than others. There’s still room to explore new approaches, especially ones that can work well with limited or adapt to new citation styles.

This is where our work comes in; we reviewed the reference parsing problem with a new perspective and, instead of solely relying on deep learning architectures, we take inspiration from \textbf{AnyStyle}, an open-source reference parser that uses a set of hand-engineered features to guide the model. These hand features show us that a well-designed feature space can go a long way in improving the performance, especially when paired with an appropriate model~\cite{anystyle}.
We explore how these kinds of features can enhance the performance of sequence models for reference parsing. Specifically, we compare traditional CRFs and BiLSTM + CRF architecture under a shared framework, but we incorporate a set of hand-engineered features inspired by AnyStyle with different types of embeddings. Our experiments use the \textbf{Giant} corpus for training and evaluating the model, ensuring that all models are tested under the same circumstances.

By analyzing performance across these models, we want to answer a key question: \textbf{Can a mixture of hand features and embeddings give us both accuracy and adaptability?}
Ultimately, our goal is to contribute a deeper understanding of how feature engineering and model design interact in this domain, and to offer practical insights for building more accurate, reliable reference parsers to be able to use in citation indexing systems.
