\section{Sequence Labeling Models}
In this section, we outline the sequence labeling models employed to parse strings of reference into structured fields. The task is posed as a token-level classification problem, with each token or word in a citation given a label like \texttt{Author}, \texttt{Title}, \texttt{Year}, etc. To address this, we look at a series of models, beginning with simple Conditional Random Fields (CRFs) and moving towards more expressive neural models like BiLSTM with CRF. Each model processes features in a different manner, and the design choices they make are in an effort to balance between accuracy, interpretability, and computational cost.
\subsection{Conditional Random Fields (CRF)}
Conditional Random Fields (CRFs) are probabilistic graphical models that are highly used in sequence labeling tasks in natural language processing (NLP). Lafferty, McCallum, and Pereira~\cite{crf2001} first introduced CRFs as modeling the conditional probability of a sequence of labels given a sequence of input observations. Unlike generative models such as Hidden Markov Models (HMMs), CRFs do not make strong independence assumptions on the input and instead optimize the conditional likelihood of the output sequence directly.

In CRFs, the label of each sequence is not only a function of the input features of the respective token but also of the labels of neighboring tokens. This makes CRFs particularly well-suited to applications where output structure is important, i.e., part-of-speech tagging, named entity recognition, and, in this work, bibliographic reference parsing. In this case, tokens in a citation string are labeled with field types (e.g., Author, Title, Journal), and capturing dependencies between consecutive labels greatly enhances prediction.

There are two implementations of CRFs discussed in this thesis:
\begin{compactitem}
\item A traditional CRF using the CRFsuite library, relying on hand-designed and subword features to facilitate field prediction with no contextual modeling.
\item A neural CRF implemented in PyTorch, acting as an output layer in a deep architecture (e.g., BiLSTM + CRF). In this implementation, the CRF is fed dense, contextual embeddings from an earlier neural encoder and learns to capture the transitions of the labels alongside the acquired representations.
\end{compactitem}
CRFs provide a principled and explainable way of solving structured prediction problems. In both versions, the CRF layer guarantees that the output labels have a valid sequence through learning of transition dependencies between output labels.
\begin{figure}[ht]
    \centering
    \input{./figures/crf.tex}
    \caption[Linear-Chain CRF for Sequence Labeling]{Graphical representation of a linear-chain CRF used for sequence labeling.}
    \label{fig:crf-chain}
\end{figure}

\subsubsection{CRFsuite}
At first deployment, Conditional Random Fields had been used using the CRFsuite library, a lightweight yet effective structpred task framework for sequence labeling and other structured tasks~\cite{pythoncrfsuite}. This was a non-contextual baseline model, and it used only rich, handcrafted features and subword-level embeddings rather than deep contextual encoders.

The model input was a mixture of
\begin{compactitem}
\item \textbf{Handcrafted features}, borrowed from the AnyStyle parser, that mimic token-level features such as affixes, capitalization, punctuation, semantic category, and position.
\item \textbf{Byte-Pair Embeddings (BPEmb)}, providing subword-level semantic information for each token for most languages.
\end{compactitem}
These features were combined into a sparse, flat feature vector for each token in a target reference string. CRFsuite does not require vector embeddings as dense tensors such as neural models, but it accepts features in the form of string-labeled attributes, and therefore it is a viable choice for traditional sequence tagging with less computational need.
CRFsuite learns label dependencies through the sequence as well as inter-field transitions like \texttt{Author}, \texttt{Title}, and \texttt{Year}. However, it does not learn any context semantics through the sequence except label dependencies; no internal representation exists of the sequence content except in local features. Despite this, the combination of handcrafted linguistic features with BPEmb provided a respectable baseline in terms of accuracy as well as generalization.
The deployment was achieved with the assistance of the Python-CRFsuite library, which is a Python binding to CRFsuite offering a clean and expressive API for describing features and training CRF models.

\subsubsection{Neural CRF in PyTorch}
The second CRF implementation, i.e., Conditional Random Fields (CRF), used in this case is a neural variant integrated in a deep learning pipeline with PyTorch~\cite{torchcrf}. Unlike the CRFsuite-based model, which is specified over sparse feature vectors only, this implementation uses CRF as the output layer of a neural network. This allows the model to learn and use dense contextual embeddings while retaining the structured output behavior of traditional CRFs.
In this setup, each token in the input sequence is represented by a concatenated embedding vector comprising:
\begin{compactitem}
\item \textbf{Pretrained subword embeddings} (e.g., BERT or BPEmb),
\item \textbf{Trainable embeddings} from handcrafted categorical features.
\end{compactitem}
The pretrained embeddings are input to a Bidirectional LSTM (BiLSTM) encoder, which captures context across the entire sequence in both the forward and backward directions. The output of the BiLSTM — a contextualized representation for each token — is concatenated with the hand features, and then processed by a linear layer that produces emission scores for each label.
On top of the final layer, there is a CRF module that decodes the most likely sequence of labels by modeling the transition between tags. The module is trained jointly with the rest of the network under a negative log-likelihood loss, allowing the model to simultaneously learn emission as well as transition parameters.
This design benefits from both learned representations and structured sequence modeling. While the BiLSTM produces contextualized input embeddings, the CRF layer ensures that label predictions are in line with common citation patterns — e.g., ensuring that a B-Author label is not directly followed by a B-Year without an intervening I-Author.

The neural CRF was implemented using the torchcrf package, a widely used CRF layer for PyTorch that offers both training and decoding using efficient forward–backward algorithms.

\subsection{BiLSTM + CRF}
The BiLSTM + CRF architecture is among the most widespread and effective models for sequence labeling tasks, particularly if both label dependencies and contextual information are relevant. In this work, the BiLSTM is used to generate contextual embeddings of each token from subword representations, and the CRF layer embeds the dependencies between the output labels so that coherent predictions can be generated.

\subsubsection{BiLSTM Encoding}
LSTM networks are recurrent neural networks (RNNs) used to process long-range dependencies using gating mechanisms regulating information flowing through time~\cite{lstm}. Bidirectional LSTM reads the sequence in both directions, accessing context from future and previous tokens for each word~\cite{bilstm}. It is particularly useful in parsing the reference string, where the label of a token can depend on both the context before and after it.
In this design, the input to the BiLSTM is subword embeddings obtained from Byte-Pair Encoding (BPEmb). They offer semantics at the subword level and generalize for citation styles and languages. The BiLSTM outputs a sequence of hidden states for every token, representing its context-aware embedding.

\subsubsection{Integration of Handcrafted Features}
In addition to the subword embeddings, manually crafted features inspired by AnyStyle~\cite{anystyle} are used. Such categorical features are processed into a trainable embedding layer and projected to the same space as the BiLSTM representations. The two representations are combined using element-wise addition to form a fused vector that includes both learned and manually crafted knowledge.

\subsubsection{CRF Output Layer}
The combined representations are fed into a linear classifier to produce emission scores for each label. These are fed as input to a CRF layer that models label transitions and produces the most probable sequence using the Viterbi algorithm. The CRF is trained on a negative log-likelihood loss, which encourages it to produce high scores for valid and correct label sequences.

This model allows the system to take advantage of both deep contextualized representations and sequence-level coherence, and it is best suited for annotating structured references with highly diverse formatting styles.

\begin{figure}[ht]
    \centering
    \input{./figures/model.tex}
    \caption[BiLSTM + CRF Model Architecture]{Architecture of the BiLSTM + CRF model combining pre-trained embeddings and handcrafted features.}
    \label{fig:bilstm-crf-architecture}
\end{figure}