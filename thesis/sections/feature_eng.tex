\section{Feature Engineering}

Feature engineering is vital to the reference parsing system introduced in this work. The bibliographic reference segmentation is a prediction problem, one where the model must assign a field-level label (author, title, year, etc.) to each token in a sequence. In order to be able to do this efficiently, the model requires informative representations of each token, those representations should capture both the token’s content and its contextual significance.

In the past few years, there has been a growing popularity in employing deep contextual embeddings like BERT~\cite{2019-bert} to embed text in natural language processing tasks. Even in tasks like citation parsing, where structure and formatting have strong semantic clues, traditional hand-engineered features can still have a significant value. They extract surface information that is often consistent across citation styles, like punctuation patterns, capitalization, or token order.

To gain a balance between generalization and interpretability, our system uses two types of features:
\begin{enumerate}
\item A set of \textbf{handcrafted features} inspired by the AnyStyle~\cite{anystyle} citation parser.
\item \textbf{Learned embeddings} derived from either subword-level models (BPE)~\cite{bpemb} or deep contextual models (BERT)~\cite{2019-bert}.
\end{enumerate}
In the following sections, we describe each feature group in detail, beginning with the handcrafted features.

\input{sections/hand_features.tex}
\clearpage

\input{sections/embeddings.tex}
\clearpage

\subsection{Feature Integration}
A key challenge in this work is integrating information from different levels of granularity: \textbf{subword-based embeddings} (e.g., BERT, BPEmb) and \textbf{word-level handcrafted features} (e.g., affix, capitalization, punctuation). This integration is handled differently depending on the model architecture.

\textbf{Neural Models (e.g., BiLSTM + CRF)}

In models that consume dense vector representations, each input token is represented by combining learned contextual embeddings with structured handcrafted features:
\begin{compactitem}
\item \textbf{Subword Embeddings:} Tokens are passed through pretrained models such as BERT or BPEmb. These models tokenize words into subword units internally (e.g., “information” may be split into "in", "\#\#formation"). Since handcrafted features operate at the word level, we align the two by averaging the embeddings of subwords corresponding to each original word token. This produces a single subword-aware vector per token.
\item \textbf{Handcrafted Features:} Each token also has categorical handcrafted features (e.g., prefix, caps type). Each of these is treated as a separate vocabulary and embedded using \texttt{nn.Embedding} layers in PyTorch. These embeddings are trained alongside the model.
\end{compactitem}
The two components are \textbf{aligned} in dimensionality: the subword embedding is passed through a BiLSTM layer, and the handcrafted feature embedding is projected via a linear layer to match the LSTM output’s dimensions. The two vectors are then added element-wise and passed to a CRF layer for structured prediction.

\textbf{Sparse CRF Models (e.g., python-crfsuite)}

In models like CRFsuite that do not support dense vector inputs, subword embeddings are converted to feature dictionaries. Each token's embedding is flattened and assigned feature names with numeric indices (e.g., \texttt{bert\_0\=0.123}). If two embedding sources are used (e.g., BERT and hand features), their features are merged using distinct prefixes. Handcrafted features, in this case, are also included as categorical string features.
Since CRFsuite works with string-based sparse features, there is no need for alignment between the two embedding sources—each word is treated as a single token, and the embedding representation of a word that contains multiple subwords is averaged beforehand during preprocessing.

This setup ensures that both contextual richness from deep embeddings and structural insight from handcrafted features are available to the model. Dense architectures learn the interaction between the two during training, while sparse CRFs receive both as complementary symbolic inputs.

