\section{Training and Evaluation Setup}
Two different training pipelines were used for the CRFsuite-based model and the BiLSTM+CRF model implemented in PyTorch. Both models were trained on processed data derived from the GIANT dataset, using subsets of varying size based on model complexity and resource constraints.
\subsection{CRFsuite Model}
The CRFsuite model was implemented using the sklearn-crfsuite Python library. Feature vectors for each token were constructed from a combination of handcrafted features and Byte-Pair Embeddings (BPEmb). These features were extracted and formatted into the string-keyed dictionary structure expected by CRFsuite.
\begin{compactitem}
\item \textbf{Training size:} 1 million \/ 5 million annotated reference strings
\item \textbf{Test size:} 100,000 \/ 200,000 references
\item \textbf{Optimizer:} Limited-memory BFGS (lbfgs)
\item \textbf{Regularization:} c1 = 0.1, c2 = 0.1
\item \textbf{Max iterations:} 100
\item \textbf{Transitions:} Enabled \texttt{all\_possible\_transitions} for richer label modeling
\end{compactitem}
The model was trained using default BIO-annotated labels. Model performance was evaluated using token-level metrics (precision, recall, F1-score) computed by flattening the sequences. The trained CRF model was serialized and stored using pickle for reuse and reproducibility.

\subsection{BiLSTM + CRF Model (Neural CRF)}
The BiLSTM + CRF model was implemented in PyTorch. It uses dense embeddings and contextual modeling, and was trained on a larger subset of the GIANT dataset due to its capacity to model long-range dependencies.
\begin{compactitem}
\item \textbf{Training size:} 5 million annotated reference strings
\item \textbf{Validation size:} 200,000 references
\item \textbf{Test size:} 200,000 references
\item \textbf{Batch size:} 32
\item \textbf{Dropout:} 0.5
\item \textbf{Optimizer:} Adam (torch.optim.Adam)
\item \textbf{Loss function:} Negative Log-Likelihood (from CRF layer)
\item \textbf{Embedding:}
\begin{compactitem}
\item Subword embeddings from BPEmb
\item Trainable embeddings from handcrafted feature classes
\end{compactitem}
\item \textbf{Architecture:}
\begin{compactitem}
\item Bidirectional LSTM followed by dropout
\item Projected handcrafted features added to LSTM output
\item Linear layer to generate emission scores
\item CRF layer for structured decoding
\end{compactitem}
\end{compactitem}

Datasets were loaded and batched using PyTorch's \texttt{DataLoader}, with a custom collate function that aligns token lengths and encodes categorical feature indices. Label-to-index mappings were handled dynamically based on the full BIO label set used in preprocessing.
Evaluation was conducted on the test set using the token-level F1-score and classification reports generated by \texttt{sklearn\.metrics\.classification\_report}.

\subsection{BERT-based CRF Model}
The BERT-based CRF models were implemented in sklearn-crfsuite Python library. Instead of training a contextual encoder from scratch, the system used pre-trained BERT-like models to extract high-quality embeddings, which were then directly fed into a CRF layer for structured prediction over tokens.

Two types of encoders were experimented with:
\begin{compactitem}
\item \textbf{Linq-Embed-Mistral:} A modern, lightweight, high-performance multilingual encoder.
\item \textbf{DistilBERT Multilingual Cased:} A distilled, faster version of multilingual BERT, providing strong token representations with reduced computational cost.
\end{compactitem}

The training setup was as follows:
\begin{compactitem}
\item \textbf{Training size:} 5 million annotated reference strings
\item \textbf{Test size:} 200,000 references
\item \textbf{Optimizer:} Limited-memory BFGS (lbfgs)
\item \textbf{Regularization:} c1 = 0.1, c2 = 0.1
\item \textbf{Max iterations:} 100
\item \textbf{Transitions:} Enabled \texttt{all\_possible\_transitions} for richer label modeling
\item \textbf{Embedding:}
\begin{compactitem}
    \item Contextual embeddings from the frozen BERT encoder
\end{compactitem}
\end{compactitem}

Datasets were processed similarly to other models. Evaluation was performed on the flattened token sequences using macro-averaged and weighted F1-scores, as reported by \texttt{sklearn.metrics.classification\_report}.
