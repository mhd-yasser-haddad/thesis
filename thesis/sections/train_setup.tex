\section{Training and Evaluation Setup}
Two different training pipelines were used for the CRFsuite-based model and the BiLSTM+CRF model implemented in PyTorch. Both models were trained on processed data derived from the GIANT dataset, using subsets of varying size based on model complexity and resource constraints.
\subsection{CRFsuite Model}
The CRFsuite model was implemented using the sklearn-crfsuite Python library. Feature vectors for each token were constructed from a combination of handcrafted features and Byte-Pair Embeddings (BPEmb). These features were extracted and formatted into the string-keyed dictionary structure expected by CRFsuite.
\begin{compactitem}
\item \textbf{Training size:} 1 million \/ 5 million annotated reference strings
\item \textbf{Test size:} 100,000 \/ 200,000 references
\item \textbf{Optimizer:} Limited-memory BFGS (lbfgs)
\item \textbf{Regularization:} c1 = 0.1, c2 = 0.1
\item \textbf{Max iterations:} 100
\item \textbf{Transitions:} Enabled \texttt{all\_possible\_transitions} for richer label modeling
\end{compactitem}
The model was trained using default BIO-annotated labels. Model performance was evaluated using token-level metrics (precision, recall, F1-score) computed by flattening the sequences. The trained CRF model was serialized and stored using pickle for reuse and reproducibility.

\subsection{BiLSTM + CRF Model (Neural CRF)}
The BiLSTM + CRF model was implemented in PyTorch. It uses dense embeddings and contextual modeling, and was trained on a larger subset of the GIANT dataset due to its capacity to model long-range dependencies.
\begin{compactitem}
\item \textbf{Training size:} 5 million annotated reference strings
\item \textbf{Validation size:} 200,000 references
\item \textbf{Test size:} 200,000 references
\item \textbf{Batch size:} 32
\item \textbf{Dropout:} 0.5
\item \textbf{Optimizer:} Adam (torch.optim.Adam)
\item \textbf{Loss function:} Negative Log-Likelihood (from CRF layer)
\item \textbf{Embedding:}
\begin{compactitem}
\item Subword embeddings from BPEmb
\item Trainable embeddings from handcrafted feature classes
\end{compactitem}
\item \textbf{Architecture:}
\begin{compactitem}
\item Bidirectional LSTM followed by dropout
\item Projected handcrafted features added to LSTM output
\item Linear layer to generate emission scores
\item CRF layer for structured decoding
\end{compactitem}
\end{compactitem}

Datasets were loaded and batched using PyTorch's \texttt{DataLoader}, with a custom collate function that aligns token lengths and encodes categorical feature indices. Label-to-index mappings were handled dynamically based on the full BIO label set used in preprocessing.
Evaluation was conducted on the test set using the token-level F1-score and classification reports generated by \texttt{sklearn\.metrics\.classification\_report}.
